{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdeUdfqGobGN6EOYrNjUcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkinapellisaicharan14/InternshalaProject/blob/main/Internshala_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-1"
      ],
      "metadata": {
        "id": "_AFYbCfJhP2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YSa8VltCf0jA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_pages = 20\n",
        "base_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\""
      ],
      "metadata": {
        "id": "B0QZcfEVf5hU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"k\": \"bags\",\n",
        "    \"crid\": \"2M096C61O4MLT\",\n",
        "    \"qid\": \"1653308124\",\n",
        "    \"sprefix\": \"ba%2Caps%2C283\",\n",
        "    \"ref\": \"sr_pg_1\"\n",
        "}"
      ],
      "metadata": {
        "id": "oPnDNbxngbT-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page in range(1, num_pages + 1):\n",
        "    params[\"ref\"] = f\"sr_pg_{page}\"\n",
        "    response = requests.get(base_url, params=params)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    \n",
        "    \n",
        "    product_listings = soup.find_all(\"div\", class_=\"s-result-item\")\n",
        "    \n",
        "    \n",
        "   "
      ],
      "metadata": {
        "id": "9olFJVG7geX_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for listing in product_listings:\n",
        "        \n",
        "        product_url = listing.find(\"a\", class_=\"a-link-normal s-no-outline\")[\"href\"]\n",
        "        \n",
        "       \n",
        "        product_name = listing.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\").text\n",
        "        \n",
        "       \n",
        "        product_price = listing.find(\"span\", class_=\"a-price-whole\").text\n",
        "        \n",
        "       \n",
        "        rating = listing.find(\"span\", class_=\"a-icon-alt\").text\n",
        "        \n",
        "        \n",
        "        num_reviews = listing.find(\"span\", class_=\"a-size-base\").text\n",
        "        \n",
        "        \n",
        "        print(\"Product URL:\", product_url)\n",
        "        print(\"Product Name:\", product_name)\n",
        "        print(\"Product Price:\", product_price)\n",
        "        print(\"Rating:\", rating)\n",
        "        print(\"Number of reviews:\", num_reviews)\n",
        "        print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "rfMl6DZ2g9-y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "part-2"
      ],
      "metadata": {
        "id": "7B0j7TkFhW6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv"
      ],
      "metadata": {
        "id": "9-CDsLEPhcxZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_urls = 200"
      ],
      "metadata": {
        "id": "msNjn4ufhfkp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\""
      ],
      "metadata": {
        "id": "KpZePngEhkEc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []"
      ],
      "metadata": {
        "id": "bEo9zVZuibqc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"k\": \"bags\",\n",
        "    \"crid\": \"2M096C61O4MLT\",\n",
        "    \"qid\": \"1653308124\",\n",
        "    \"sprefix\": \"ba%2Caps%2C283\",\n",
        "    \"ref\": \"sr_pg_1\"\n",
        "}"
      ],
      "metadata": {
        "id": "hRlRL2_Jn-rm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_urls = []"
      ],
      "metadata": {
        "id": "Fy-Csa5Fn_8-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for url in product_urls[:num_urls]:\n",
        "    # Send a GET request to the product URL\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    \n",
        "    # Extract the required information from the product page\n",
        "    product_details = {}\n",
        "    \n",
        "    # Product URL\n",
        "    product_details[\"Product URL\"] = url\n",
        "    \n",
        "    # Description\n",
        "    description = soup.find(\"span\", id=\"productTitle\")\n",
        "    product_details[\"Description\"] = description.text.strip() if description else \"\"\n",
        "    \n",
        "    # ASIN\n",
        "    asin = soup.find(\"th\", string=\"ASIN\")\n",
        "    product_details[\"ASIN\"] = asin.find_next_sibling(\"td\").text.strip() if asin else \"\"\n",
        "    \n",
        "    # Product Description\n",
        "    product_desc = soup.find(\"div\", id=\"productDescription\")\n",
        "    product_details[\"Product Description\"] = product_desc.text.strip() if product_desc else \"\"\n",
        "    \n",
        "    # Manufacturer\n",
        "    manufacturer = soup.find(\"a\", id=\"bylineInfo\")\n",
        "    product_details[\"Manufacturer\"] = manufacturer.text.strip() if manufacturer else \"\"\n",
        "    \n",
        "    # Append the scraped data to the list\n",
        "    data.append(product_details)\n",
        "\n",
        "# Export the data to a CSV file\n",
        "filename = \"product_data.csv\"\n",
        "\n",
        "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)"
      ],
      "metadata": {
        "id": "_CXHEd1Doypi"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}